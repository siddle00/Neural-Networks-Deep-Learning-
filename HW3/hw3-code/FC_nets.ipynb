{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected networks\n",
    "\n",
    "In the previous notebook, you implemented a simple two-layer neural network class.  However, this class is not modular.  If you wanted to change the number of layers, you would need to write a new loss and gradient function.  If you wanted to optimize the network with different optimizers, you'd need to write new training functions.  If you wanted to incorporate regularizations, you'd have to modify the loss and gradient function.  \n",
    "\n",
    "Instead of having to modify functions each time, for the rest of the class, we'll work in a more modular framework where we define forward and backward layers that calculate losses and gradients respectively.  Since the forward and backward layers share intermediate values that are useful for calculating both the loss and the gradient, we'll also have these function return \"caches\" which store useful intermediate values.\n",
    "\n",
    "The goal is that through this modular design, we can build different sized neural networks for various applications.\n",
    "\n",
    "In this HW #3, we'll define the basic architecture, and in HW #4, we'll build on this framework to implement different optimizers and regularizations (like BatchNorm and Dropout).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular layers\n",
    "\n",
    "This notebook will build modular layers in the following manner.  First, there will be a forward pass for a given layer with inputs (`x`) and return the output of that layer (`out`) as well as cached variables (`cache`) that will be used to calculate the gradient in the backward pass.\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import and setups\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nndl.fc_net import *\n",
    "from utils.data_utils import get_CIFAR10_data\n",
    "from utils.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from utils.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (49000, 3, 32, 32) \n",
      "y_train: (49000,) \n",
      "X_val: (1000, 3, 32, 32) \n",
      "y_val: (1000,) \n",
      "X_test: (1000, 3, 32, 32) \n",
      "y_test: (1000,) \n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k in data.keys():\n",
    "  print('{}: {} '.format(k, data[k].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layers\n",
    "\n",
    "In this section, we'll implement the forward and backward pass for the linear layers.\n",
    "\n",
    "The linear layer forward pass is the function `affine_forward` in `nndl/layers.py` and the backward pass is `affine_backward`.\n",
    "\n",
    "After you have implemented these, test your implementation by running the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine layer forward pass\n",
    "\n",
    "Implement `affine_forward` and then test your code by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference: 9.7698500479884e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: {}'.format(rel_error(out, correct_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine layer backward pass\n",
    "\n",
    "Implement `affine_backward` and then test your code by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error: 8.393418893437644e-11\n",
      "dw error: 5.789532070351032e-11\n",
      "db error: 4.358966478791854e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: {}'.format(rel_error(dx_num, dx)))\n",
    "print('dw error: {}'.format(rel_error(dw_num, dw)))\n",
    "print('db error: {}'.format(rel_error(db_num, db)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation layers\n",
    "\n",
    "In this section you'll implement the ReLU activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU forward pass\n",
    "\n",
    "Implement the `relu_forward` function in `nndl/layers.py` and then test your code by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference: 4.999999798022158e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: {}'.format(rel_error(out, correct_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU backward pass\n",
    "Implement the `relu_backward` function in `nndl/layers.py` and then test your code by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error: 3.275624447877486e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: {}'.format(rel_error(dx_num, dx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the affine and ReLU layers\n",
    "\n",
    "Often times, an affine layer will be followed by a ReLU layer. So let's make one that puts them together.  Layers that are combined are stored in `nndl/layer_utils.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine-ReLU layers\n",
    "We've implemented `affine_relu_forward()` and `affine_relu_backward` in ``nndl/layer_utils.py``.  Take a look at them to make sure you understand what's going on.  Then run the following cell to ensure its implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_relu_forward and affine_relu_backward:\n",
      "dx error: 2.237042962557352e-10\n",
      "dw error: 2.161394306810927e-09\n",
      "db error: 7.826729316297815e-12\n"
     ]
    }
   ],
   "source": [
    "from nndl.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "print('Testing affine_relu_forward and affine_relu_backward:')\n",
    "print('dx error: {}'.format(rel_error(dx_num, dx)))\n",
    "print('dw error: {}'.format(rel_error(dw_num, dw)))\n",
    "print('db error: {}'.format(rel_error(db_num, db)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax losses\n",
    "\n",
    "You've already implemented it, so we have written it in `layers.py`.  The following code will ensure its working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing softmax_loss:\n",
      "loss: 2.3025992297115487\n",
      "dx error: 8.659001943244728e-09\n"
     ]
    }
   ],
   "source": [
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: {}'.format(loss))\n",
    "print('dx error: {}'.format(rel_error(dx_num, dx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a two-layer NN\n",
    "\n",
    "In `nndl/fc_net.py`, implement the class `TwoLayerNet` which uses the layers you made here.  When you have finished, the following cell will test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "Running numeric gradient check with reg = 0.0\n",
      "W1 relative error: 1.2236151215593397e-08\n",
      "W2 relative error: 3.3429539606923665e-10\n",
      "b1 relative error: 4.7288944058018464e-09\n",
      "b2 relative error: 4.3291285233961314e-10\n",
      "Running numeric gradient check with reg = 0.7\n",
      "W1 relative error: 2.527915286171985e-07\n",
      "W2 relative error: 1.3678335722105113e-07\n",
      "b1 relative error: 1.5646801749611563e-08\n",
      "b2 relative error: 9.089621155678095e-10\n"
     ]
    }
   ],
   "source": [
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-2\n",
    "model = TwoLayerNet(input_dim=D, hidden_dims=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = {}'.format(reg))\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('{} relative error: {}'.format(name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver \n",
    "\n",
    "We will now use the utils Solver class to train these networks.  Familiarize yourself with the API in `utils/solver.py`.  After you have done so, declare an instance of a TwoLayerNet with 200 units and then train it with the Solver.  Choose parameters so that your validation accuracy is at least 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 2270) loss: 2.396164\n",
      "(Epoch 0 / 10) train acc: 0.127000; val_acc: 0.123000\n",
      "(Iteration 101 / 2270) loss: 1.848692\n",
      "(Iteration 201 / 2270) loss: 1.817794\n",
      "(Epoch 1 / 10) train acc: 0.440000; val_acc: 0.429000\n",
      "(Iteration 301 / 2270) loss: 1.663915\n",
      "(Iteration 401 / 2270) loss: 1.652731\n",
      "(Epoch 2 / 10) train acc: 0.472000; val_acc: 0.480000\n",
      "(Iteration 501 / 2270) loss: 1.681828\n",
      "(Iteration 601 / 2270) loss: 1.552158\n",
      "(Epoch 3 / 10) train acc: 0.515000; val_acc: 0.465000\n",
      "(Iteration 701 / 2270) loss: 1.631960\n",
      "(Iteration 801 / 2270) loss: 1.484787\n",
      "(Iteration 901 / 2270) loss: 1.586235\n",
      "(Epoch 4 / 10) train acc: 0.506000; val_acc: 0.471000\n",
      "(Iteration 1001 / 2270) loss: 1.486124\n",
      "(Iteration 1101 / 2270) loss: 1.434921\n",
      "(Epoch 5 / 10) train acc: 0.543000; val_acc: 0.511000\n",
      "(Iteration 1201 / 2270) loss: 1.356191\n",
      "(Iteration 1301 / 2270) loss: 1.462831\n",
      "(Epoch 6 / 10) train acc: 0.547000; val_acc: 0.513000\n",
      "(Iteration 1401 / 2270) loss: 1.301775\n",
      "(Iteration 1501 / 2270) loss: 1.320279\n",
      "(Epoch 7 / 10) train acc: 0.553000; val_acc: 0.497000\n",
      "(Iteration 1601 / 2270) loss: 1.366449\n",
      "(Iteration 1701 / 2270) loss: 1.385570\n",
      "(Iteration 1801 / 2270) loss: 1.342451\n",
      "(Epoch 8 / 10) train acc: 0.563000; val_acc: 0.529000\n",
      "(Iteration 1901 / 2270) loss: 1.313338\n",
      "(Iteration 2001 / 2270) loss: 1.534167\n",
      "(Epoch 9 / 10) train acc: 0.562000; val_acc: 0.521000\n",
      "(Iteration 2101 / 2270) loss: 1.400173\n",
      "(Iteration 2201 / 2270) loss: 1.350052\n",
      "(Epoch 10 / 10) train acc: 0.609000; val_acc: 0.540000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet()\n",
    "solver = None\n",
    "\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "#   Declare an instance of a TwoLayerNet and then train \n",
    "#   it with the Solver. Choose hyperparameters so that your validation \n",
    "#   accuracy is at least 50%.  We won't have you optimize this further\n",
    "#   since you did it in the previous notebook.\n",
    "#\n",
    "# ================================================================ #\n",
    "\n",
    "model = TwoLayerNet(hidden_dims=200, reg = 0.3)\n",
    "solver = Solver(model, data,\n",
    "                  update_rule='sgd',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 8.5*1e-4,\n",
    "                  },\n",
    "                  lr_decay=0.95,\n",
    "                  num_epochs=10, batch_size=215,\n",
    "                  print_every=100)\n",
    "solver.train()\n",
    "\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Neural Network\n",
    "\n",
    "Now, we implement a multi-layer neural network.\n",
    "\n",
    "Read through the `FullyConnectedNet` class in the file `nndl/fc_net.py`.\n",
    "\n",
    "Implement the initialization, the forward pass, and the backward pass.  There will be lines for batchnorm and dropout layers and caches; ignore these all for now.  That'll be in HW #4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg = 0\n",
      "Initial loss: 2.3031869828794544\n",
      "W1 relative error: 1.2897016272675205e-06\n",
      "W2 relative error: 2.812817056147554e-06\n",
      "W3 relative error: 1.4289315756613801e-06\n",
      "b1 relative error: 2.831173759131293e-08\n",
      "b2 relative error: 9.975336956622204e-09\n",
      "b3 relative error: 1.2105988878862188e-10\n",
      "Running check with reg = 3.14\n",
      "Initial loss: 7.206316830595218\n",
      "W1 relative error: 1.3895236383682937e-08\n",
      "W2 relative error: 1.3692782141902887e-07\n",
      "W3 relative error: 1.4240049661228542e-08\n",
      "b1 relative error: 3.3103688355403096e-08\n",
      "b2 relative error: 4.529979014602335e-09\n",
      "b3 relative error: 1.6222947517855002e-10\n"
     ]
    }
   ],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = {}'.format(reg))\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: {}'.format(loss))\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('{} relative error: {}'.format(name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: 3.446991\n",
      "(Epoch 0 / 20) train acc: 0.240000; val_acc: 0.090000\n",
      "(Epoch 1 / 20) train acc: 0.280000; val_acc: 0.116000\n",
      "(Epoch 2 / 20) train acc: 0.560000; val_acc: 0.151000\n",
      "(Epoch 3 / 20) train acc: 0.680000; val_acc: 0.141000\n",
      "(Epoch 4 / 20) train acc: 0.740000; val_acc: 0.183000\n",
      "(Epoch 5 / 20) train acc: 0.860000; val_acc: 0.173000\n",
      "(Iteration 11 / 40) loss: 0.587222\n",
      "(Epoch 6 / 20) train acc: 0.880000; val_acc: 0.180000\n",
      "(Epoch 7 / 20) train acc: 0.940000; val_acc: 0.175000\n",
      "(Epoch 8 / 20) train acc: 0.940000; val_acc: 0.187000\n",
      "(Epoch 9 / 20) train acc: 0.940000; val_acc: 0.191000\n",
      "(Epoch 10 / 20) train acc: 0.940000; val_acc: 0.182000\n",
      "(Iteration 21 / 40) loss: 0.240758\n",
      "(Epoch 11 / 20) train acc: 0.980000; val_acc: 0.175000\n",
      "(Epoch 12 / 20) train acc: 1.000000; val_acc: 0.182000\n",
      "(Epoch 13 / 20) train acc: 1.000000; val_acc: 0.182000\n",
      "(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.185000\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.179000\n",
      "(Iteration 31 / 40) loss: 0.103256\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.182000\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.175000\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.182000\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.181000\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.181000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the three layer neural network to overfit a small dataset.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "\n",
    "#### !!!!!!\n",
    "# Play around with the weight_scale and learning_rate so that you can overfit a small dataset.\n",
    "# Your training accuracy should be 1.0 to receive full credit on this part.\n",
    "weight_scale = 2e-2\n",
    "learning_rate = 3e-3\n",
    "\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def affine_forward(x, w, b):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "  The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "  examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "  reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "  then transform it to an output vector of dimension M.\n",
    "\n",
    "  Inputs:\n",
    "  - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "  - w: A numpy array of weights, of shape (D, M)\n",
    "  - b: A numpy array of biases, of shape (M,)\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - out: output, of shape (N, M)\n",
    "  - cache: (x, w, b)\n",
    "  \"\"\"\n",
    "\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Calculate the output of the forward pass.  Notice the dimensions\n",
    "  #   of w are D x M, which is the transpose of what we did in earlier \n",
    "  #   assignments.\n",
    "  # ================================================================ #\n",
    "\n",
    "  x_trans = x.reshape(x.shape[0], -1) \n",
    "  out = np.dot(x_trans,w) + b\n",
    "\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ #\n",
    "    \n",
    "  cache = (x, w, b)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for an affine layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivative, of shape (N, M)\n",
    "  - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "  - dw: Gradient with respect to w, of shape (D, M)\n",
    "  - db: Gradient with respect to b, of shape (M,)\n",
    "  \"\"\"\n",
    "  x, w, b = cache\n",
    "  dx, dw, db = None, None, None\n",
    "\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Calculate the gradients for the backward pass.\n",
    "  # ================================================================ #\n",
    "\n",
    "  # dout is N x M\n",
    "  # dx should be N x d1 x ... x dk; it relates to dout through multiplication with w, which is D x M\n",
    "  # dw should be D x M; it relates to dout through multiplication with x, which is N x D after reshaping\n",
    "  # db should be M; it is just the sum over dout examples\n",
    "\n",
    "  x_trans = x.reshape(x.shape[0], -1)   \n",
    "  dx = np.dot(dout,w.T)\n",
    "  dx = dx.reshape(x.shape)\n",
    "  dw = np.dot(x_trans.T,dout)\n",
    "  db = np.sum(dout,axis=0)\n",
    "\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ #\n",
    "  \n",
    "  return dx, dw, db\n",
    "\n",
    "def relu_forward(x):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "  Input:\n",
    "  - x: Inputs, of any shape\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: Output, of the same shape as x\n",
    "  - cache: x\n",
    "  \"\"\"\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Implement the ReLU forward pass.\n",
    "  # ================================================================ #\n",
    "\n",
    "  f = lambda x: x * (x > 0) \n",
    "  out = f(x)\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ #\n",
    " \n",
    "  cache = x\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "  Input:\n",
    "  - dout: Upstream derivatives, of any shape\n",
    "  - cache: Input x, of same shape as dout\n",
    "\n",
    "  Returns:\n",
    "  - dx: Gradient with respect to x\n",
    "  \"\"\"\n",
    "  x = cache\n",
    "\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Implement the ReLU backward pass\n",
    "  # ================================================================ #\n",
    "\n",
    "  # ReLU directs linearly to those > 0\n",
    "  x_trans = x.reshape(x.shape[0], -1) \n",
    "  ones = np.ones(dout.shape)\n",
    "  dx = dout * (x_trans >= 0)\n",
    "    \n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ #\n",
    " \n",
    "  return dx\n",
    "\n",
    "def svm_loss(x, y):\n",
    "  \"\"\"\n",
    "  Computes the loss and gradient using for multiclass SVM classification.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "  - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss: Scalar giving the loss\n",
    "  - dx: Gradient of the loss with respect to x\n",
    "  \"\"\"\n",
    "  N = x.shape[0]\n",
    "  correct_class_scores = x[np.arange(N), y]\n",
    "  margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0)\n",
    "  margins[np.arange(N), y] = 0\n",
    "  loss = np.sum(margins) / N\n",
    "  num_pos = np.sum(margins > 0, axis=1)\n",
    "  dx = np.zeros_like(x)\n",
    "  dx[margins > 0] = 1\n",
    "  dx[np.arange(N), y] -= num_pos\n",
    "  dx /= N\n",
    "  return loss, dx\n",
    "\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "  \"\"\"\n",
    "  Computes the loss and gradient for softmax classification.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "  - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss: Scalar giving the loss\n",
    "  - dx: Gradient of the loss with respect to x\n",
    "  \"\"\"\n",
    "\n",
    "  probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "  probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "  N = x.shape[0]\n",
    "  loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n",
    "  dx = probs.copy()\n",
    "  dx[np.arange(N), y] -= 1\n",
    "  dx /= N\n",
    "  return loss, dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural_net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "  \"\"\"\n",
    "  A two-layer fully-connected neural network. The net has an input dimension of\n",
    "  N, a hidden layer dimension of H, and performs classification over C classes.\n",
    "  We train the network with a softmax loss function and L2 regularization on the\n",
    "  weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
    "  connected layer.\n",
    "\n",
    "  In other words, the network has the following architecture:\n",
    "\n",
    "  input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "\n",
    "  The outputs of the second fully-connected layer are the scores for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "    \"\"\"\n",
    "    Initialize the model. Weights are initialized to small random values and\n",
    "    biases are initialized to zero. Weights and biases are stored in the\n",
    "    variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "    W1: First layer weights; has shape (H, D)\n",
    "    b1: First layer biases; has shape (H,)\n",
    "    W2: Second layer weights; has shape (C, H)\n",
    "    b2: Second layer biases; has shape (C,)\n",
    "\n",
    "    Inputs:\n",
    "    - input_size: The dimension D of the input data.\n",
    "    - hidden_size: The number of neurons H in the hidden layer.\n",
    "    - output_size: The number of classes C.\n",
    "    \"\"\"\n",
    "    self.params = {}\n",
    "    self.params['W1'] = std * np.random.randn(hidden_size, input_size)\n",
    "    self.params['b1'] = np.zeros(hidden_size)\n",
    "    self.params['W2'] = std * np.random.randn(output_size, hidden_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "\n",
    "  def loss(self, X, y=None, reg=0.0):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients for a two layer fully connected neural\n",
    "    network.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "      an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "      is not passed then we only return scores, and if it is passed then we\n",
    "      instead return the loss and gradients.\n",
    "    - reg: Regularization strength.\n",
    "\n",
    "    Returns:\n",
    "    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "    the score for class c on input X[i].\n",
    "\n",
    "    If y is not None, instead return a tuple of:\n",
    "    - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "      samples.\n",
    "    - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "      with respect to the loss function; has the same keys as self.params.\n",
    "    \"\"\"\n",
    "    # Unpack variables from the params dictionary\n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Compute the forward pass\n",
    "    scores = None\n",
    "\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the output scores of the neural network.  The result\n",
    "    #   should be (N, C). As stated in the description for this class,\n",
    "    #   there should not be a ReLU layer after the second FC layer.\n",
    "    #   The output of the second FC layer is the output scores. Do not\n",
    "    #   use a for loop in your implementation.\n",
    "    # ================================================================ #\n",
    "\n",
    "    relu = lambda x: np.maximum(0, x)\n",
    "    H1 = relu(X@(W1.T) + b1) # X is (5,4) W1.T (4,10) -> (5,10) \n",
    "    scores = H1@W2.T + b2 # H1 is (5,10), W2 is (10,10) -> (5,10)\n",
    "    \n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "  \n",
    "    # If the targets are not given then jump out, we're done\n",
    "    if y is None:\n",
    "      return scores\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = None\n",
    "\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the loss of the neural network.  This includes the\n",
    "    #   softmax loss and the L2 regularization for W1 and W2. Store the\n",
    "    #   total loss in teh variable loss.  Multiply the regularization\n",
    "    #   loss by 0.5 (in addition to the factor reg).\n",
    "    # ================================================================ #\n",
    "\n",
    "    # scores is num_examples by num_classes\n",
    "    \n",
    "    class_probabilities = np.exp(scores)/np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "    prob_of_correct_y = class_probabilities[np.arange(N), y]\n",
    "    log_loss = -np.log(prob_of_correct_y)\n",
    "    sum_log_loss = np.sum(log_loss)\n",
    "    loss = sum_log_loss/N\n",
    "    \n",
    "    frob_norm_w1 = np.sum(W1**2)\n",
    "    frob_norm_w2 = np.sum(W2**2)\n",
    "    reg_w1 = 0.5*reg*frob_norm_w1\n",
    "    reg_w2 = 0.5*reg*frob_norm_w2\n",
    "    regularized_loss = reg_w1 + reg_w2\n",
    "    \n",
    "    loss += regularized_loss \n",
    "    \n",
    "    \n",
    "    #reg_loss = 0.5 * reg * (np.linalg.norm(W1, 'fro')**2 + np.linalg.norm(W2, 'fro')**2)\n",
    "    #loss = (np.sum(-np.log(np.exp(scores[np.arange(N), y]) / np.sum(np.exp(scores), axis = 1))))/ N + reg_loss\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Implement the backward pass.  Compute the derivatives of the\n",
    "    #   weights and the biases.  Store the results in the grads\n",
    "    #   dictionary.  e.g., grads['W1'] should store the gradient for\n",
    "    #   W1, and be of the same size as W1.\n",
    "    # ================================================================ #\n",
    "\n",
    "    update_scores = class_probabilities\n",
    "    update_scores[np.arange(N), y] -=1\n",
    "    update_scores /= N\n",
    "    \n",
    "    grads['W2'] = np.dot(H1.T, update_scores).T\n",
    "    grads['b2'] = np.sum(update_scores, axis=0)\n",
    "    dH2 = np.dot(update_scores, W2)\n",
    "    \n",
    "    dLdA = dH2\n",
    "    dLdA[H1 <= 0] = 0\n",
    "\n",
    "    grads['W1'] = np.dot(dLdA.T, X)\n",
    "    grads['b1'] = np.sum(dLdA, axis=0)#, keepdims=True)\n",
    "    \n",
    "    grads['W2'] += reg * W2\n",
    "    grads['W1'] += reg * W1\n",
    "    \n",
    "\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss, grads\n",
    "\n",
    "  def train(self, X, y, X_val, y_val,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this neural network using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving training data.\n",
    "    - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
    "      X[i] has label c, where 0 <= c < C.\n",
    "    - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "    - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "    - learning_rate: Scalar giving learning rate for optimization.\n",
    "    - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "      after each epoch.\n",
    "    - reg: Scalar giving regularization strength.\n",
    "    - num_iters: Number of steps to take when optimizing.\n",
    "    - batch_size: Number of training examples to use per step.\n",
    "    - verbose: boolean; if true print progress during optimization.\n",
    "    \"\"\"\n",
    "    num_train = X.shape[0]\n",
    "    iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "    # Use SGD to optimize the parameters in self.model\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for it in np.arange(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      # ================================================================ #\n",
    "      # YOUR CODE HERE:\n",
    "      #   Create a minibatch by sampling batch_size samples randomly.\n",
    "      # ================================================================ #\n",
    "        \n",
    "      random_indices = np.random.choice(np.arange(num_train), batch_size)\n",
    "      X_batch = X[random_indices]\n",
    "      y_batch = y[random_indices]\n",
    "\n",
    "      # ================================================================ #\n",
    "      # END YOUR CODE HERE\n",
    "      # ================================================================ #\n",
    "\n",
    "       # Compute loss and gradients using the current minibatch\n",
    "      loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      # ================================================================ #\n",
    "      # YOUR CODE HERE:\n",
    "      #   Perform a gradient descent step using the minibatch to update\n",
    "      #   all parameters (i.e., W1, W2, b1, and b2).\n",
    "      # ================================================================ #\n",
    "\n",
    "      self.params['W2'] += -learning_rate * grads['W2']\n",
    "      self.params['W1'] += -learning_rate * grads['W1']\n",
    "\n",
    "      self.params['b2'] += -learning_rate * grads['b2']\n",
    "      self.params['b1'] += -learning_rate * grads['b1']\n",
    "\n",
    "      # ================================================================ #\n",
    "      # END YOUR CODE HERE\n",
    "      # ================================================================ #\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('iteration {} / {}: loss {}'.format(it, num_iters, loss))\n",
    "\n",
    "      # Every epoch, check train and val accuracy and decay learning rate.\n",
    "      if it % iterations_per_epoch == 0:\n",
    "        # Check accuracy\n",
    "        train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "        val_acc = (self.predict(X_val) == y_val).mean()\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        # Decay learning rate\n",
    "        learning_rate *= learning_rate_decay\n",
    "\n",
    "    return {\n",
    "      'loss_history': loss_history,\n",
    "      'train_acc_history': train_acc_history,\n",
    "      'val_acc_history': val_acc_history,\n",
    "    }\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this two-layer network to predict labels for\n",
    "    data points. For each data point we predict scores for each of the C\n",
    "    classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "      classify.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "      to have class c, where 0 <= c < C.\n",
    "    \"\"\"\n",
    "    y_pred = None\n",
    "\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Predict the class given the input data.\n",
    "    # ================================================================ #\n",
    "    \n",
    "    num_examples = X.shape[0]\n",
    "    y_pred = np.empty((num_examples,), dtype=int)\n",
    "    H1_preact = np.dot(X, self.params['W1'].T) + self.params['b1']\n",
    "\n",
    "    #apply RELU\n",
    "    H1  = np.maximum(0, H1_preact)\n",
    "\n",
    "    #second layer\n",
    "    H2  = np.dot(H1, self.params['W2'].T) + self.params['b2']\n",
    "\n",
    "    #apply softmax\n",
    "    softmax = np.exp(H2)/np.sum(np.exp(H2), axis=1, keepdims=True)\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        max_index = np.argmax(softmax[i])\n",
    "        y_pred[i] = max_index\n",
    "\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fc_net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from .layers import *\n",
    "from .layer_utils import *\n",
    "\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "  \"\"\"\n",
    "  A two-layer fully-connected neural network with ReLU nonlinearity and\n",
    "  softmax loss that uses a modular layer design. We assume an input dimension\n",
    "  of D, a hidden dimension of H, and perform classification over C classes.\n",
    "  \n",
    "  The architecure should be affine - relu - affine - softmax.\n",
    "\n",
    "  Note that this class does not implement gradient descent; instead, it\n",
    "  will interact with a separate Solver object that is responsible for running\n",
    "  optimization.\n",
    "\n",
    "  The learnable parameters of the model are stored in the dictionary\n",
    "  self.params that maps parameter names to numpy arrays.\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, input_dim=3*32*32, hidden_dims=100, num_classes=10,\n",
    "               dropout=0, weight_scale=1e-3, reg=0.0):\n",
    "    \"\"\"\n",
    "    Initialize a new network.\n",
    "\n",
    "    Inputs:\n",
    "    - input_dim: An integer giving the size of the input\n",
    "    - hidden_dims: An integer giving the size of the hidden layer\n",
    "    - num_classes: An integer giving the number of classes to classify\n",
    "    - dropout: Scalar between 0 and 1 giving dropout strength.\n",
    "    - weight_scale: Scalar giving the standard deviation for random\n",
    "      initialization of the weights.\n",
    "    - reg: Scalar giving L2 regularization strength.\n",
    "    \"\"\"\n",
    "    self.params = {}\n",
    "    self.reg = reg\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Initialize W1, W2, b1, and b2.  Store these as self.params['W1'], \n",
    "    #   self.params['W2'], self.params['b1'] and self.params['b2']. The\n",
    "    #   biases are initialized to zero and the weights are initialized\n",
    "    #   so that each parameter has mean 0 and standard deviation weight_scale.\n",
    "    #   The dimensions of W1 should be (input_dim, hidden_dim) and the\n",
    "    #   dimensions of W2 should be (hidden_dims, num_classes)\n",
    "    # ================================================================ #\n",
    "\n",
    "    W1_shape = (input_dim, hidden_dims)\n",
    "    W2_shape = (hidden_dims,num_classes)\n",
    "    \n",
    "    self.params['W1'] = np.random.normal(loc=0.0,scale=weight_scale,size = W1_shape)\n",
    "    self.params['b1'] = np.zeros(hidden_dims)\n",
    "    self.params['W2'] = np.random.normal(loc=0.0,scale=weight_scale,size = W2_shape)\n",
    "    self.params['b2'] = np.zeros(num_classes)\n",
    "\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "  def loss(self, X, y=None):\n",
    "    \"\"\"\n",
    "    Compute loss and gradient for a minibatch of data.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Array of input data of shape (N, d_1, ..., d_k)\n",
    "    - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
    "\n",
    "    Returns:\n",
    "    If y is None, then run a test-time forward pass of the model and return:\n",
    "    - scores: Array of shape (N, C) giving classification scores, where\n",
    "      scores[i, c] is the classification score for X[i] and class c.\n",
    "\n",
    "    If y is not None, then run a training-time forward and backward pass and\n",
    "    return a tuple of:\n",
    "    - loss: Scalar value giving the loss\n",
    "    - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "      names to gradients of the loss with respect to those parameters.\n",
    "    \"\"\"  \n",
    "    scores = None\n",
    "\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Implement the forward pass of the two-layer neural network. Store\n",
    "    #   the class scores as the variable 'scores'.  Be sure to use the layers\n",
    "    #   you prior implemented.\n",
    "    # ================================================================ #    \n",
    "    \n",
    "    W1 = self.params['W1']\n",
    "    b1 = self.params['b1']\n",
    "    W2 = self.params['W2']\n",
    "    b2 = self.params['b2']\n",
    "    \n",
    "    H, cache_h = affine_relu_forward(X, W1, b1)\n",
    "    Z, cache_z = affine_forward(H, W2, b2)\n",
    "    \n",
    "    scores = Z\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "    \n",
    "    # If y is None then we are in test mode so just return scores\n",
    "    if y is None:\n",
    "      return scores\n",
    "    \n",
    "    loss, grads = 0, {}\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Implement the backward pass of the two-layer neural net.  Store\n",
    "    #   the loss as the variable 'loss' and store the gradients in the \n",
    "    #   'grads' dictionary.  For the grads dictionary, grads['W1'] holds\n",
    "    #   the gradient for W1, grads['b1'] holds the gradient for b1, etc.\n",
    "    #   i.e., grads[k] holds the gradient for self.params[k].\n",
    "    #\n",
    "    #   Add L2 regularization, where there is an added cost 0.5*self.reg*W^2\n",
    "    #   for each W.  Be sure to include the 0.5 multiplying factor to \n",
    "    #   match our implementation.\n",
    "    #\n",
    "    #   And be sure to use the layers you prior implemented.\n",
    "    # ================================================================ #    \n",
    "    \n",
    "    loss, dz = softmax_loss(scores, y)\n",
    "    loss += 0.5 * self.reg * (np.sum(W1*W1) + np.sum(W2*W2))\n",
    "\n",
    "    dh, dw2, db2 = affine_backward(dz, cache_z)\n",
    "    dx, dw1, db1 = affine_relu_backward(dh, cache_h)\n",
    "\n",
    "    grads['W1'] = dw1 + self.reg * W1\n",
    "    grads['b1'] = db1\n",
    "    grads['W2'] = dw2 + self.reg * W2\n",
    "    grads['b2'] = db2\n",
    "\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "class FullyConnectedNet(object):\n",
    "  \"\"\"\n",
    "  A fully-connected neural network with an arbitrary number of hidden layers,\n",
    "  ReLU nonlinearities, and a softmax loss function. This will also implement\n",
    "  dropout and batch normalization as options. For a network with L layers,\n",
    "  the architecture will be\n",
    "  \n",
    "  {affine - [batch norm] - relu - [dropout]} x (L - 1) - affine - softmax\n",
    "  \n",
    "  where batch normalization and dropout are optional, and the {...} block is\n",
    "  repeated L - 1 times.\n",
    "  \n",
    "  Similar to the TwoLayerNet above, learnable parameters are stored in the\n",
    "  self.params dictionary and will be learned using the Solver class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10,\n",
    "               dropout=0, use_batchnorm=False, reg=0.0,\n",
    "               weight_scale=1e-2, dtype=np.float32, seed=None):\n",
    "    \"\"\"\n",
    "    Initialize a new FullyConnectedNet.\n",
    "    \n",
    "    Inputs:\n",
    "    - hidden_dims: A list of integers giving the size of each hidden layer.\n",
    "    - input_dim: An integer giving the size of the input.\n",
    "    - num_classes: An integer giving the number of classes to classify.\n",
    "    - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=0 then\n",
    "      the network should not use dropout at all.\n",
    "    - use_batchnorm: Whether or not the network should use batch normalization.\n",
    "    - reg: Scalar giving L2 regularization strength.\n",
    "    - weight_scale: Scalar giving the standard deviation for random\n",
    "      initialization of the weights.\n",
    "    - dtype: A numpy datatype object; all computations will be performed using\n",
    "      this datatype. float32 is faster but less accurate, so you should use\n",
    "      float64 for numeric gradient checking.\n",
    "    - seed: If not None, then pass this random seed to the dropout layers. This\n",
    "      will make the dropout layers deteriminstic so we can gradient check the\n",
    "      model.\n",
    "    \"\"\"\n",
    "    self.use_batchnorm = use_batchnorm\n",
    "    self.use_dropout = dropout > 0\n",
    "    self.reg = reg\n",
    "    self.num_layers = 1 + len(hidden_dims)\n",
    "    self.dtype = dtype\n",
    "    self.params = {}\n",
    "\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Initialize all parameters of the network in the self.params dictionary.\n",
    "    #   The weights and biases of layer 1 are W1 and b1; and in general the \n",
    "    #   weights and biases of layer i are Wi and bi. The\n",
    "    #   biases are initialized to zero and the weights are initialized\n",
    "    #   so that each parameter has mean 0 and standard deviation weight_scale.\n",
    "    # ================================================================ #\n",
    "    \n",
    "    for i in np.arange(1,self.num_layers+1):\n",
    "        \n",
    "        name_W = 'W'+str(i)\n",
    "        name_b = 'b'+str(i)\n",
    "        \n",
    "        if i == 1:                   # first layer\n",
    "            self.params[name_W] = np.random.normal(loc=0.0,scale=weight_scale,size = (input_dim,hidden_dims[i-1]))\n",
    "            self.params[name_b] = np.zeros(hidden_dims[i-1]) \n",
    "        elif i == self.num_layers:         # last layer\n",
    "            self.params[name_W] = np.random.normal(loc=0.0,scale=weight_scale,size = (hidden_dims[i-2],num_classes))\n",
    "            self.params[name_b] = np.zeros(num_classes) \n",
    "        else:                        # intermediate layers\n",
    "            self.params[name_W] = np.random.normal(loc=0.0,scale=weight_scale,size = (hidden_dims[i-2],hidden_dims[i-1]))\n",
    "            self.params[name_b] = np.zeros(hidden_dims[i-1])\n",
    "\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "    \n",
    "    # When using dropout we need to pass a dropout_param dictionary to each\n",
    "    # dropout layer so that the layer knows the dropout probability and the mode\n",
    "    # (train / test). You can pass the same dropout_param to each dropout layer.\n",
    "    self.dropout_param = {}\n",
    "    if self.use_dropout:\n",
    "      self.dropout_param = {'mode': 'train', 'p': dropout}\n",
    "      if seed is not None:\n",
    "        self.dropout_param['seed'] = seed\n",
    "    \n",
    "    # With batch normalization we need to keep track of running means and\n",
    "    # variances, so we need to pass a special bn_param object to each batch\n",
    "    # normalization layer. You should pass self.bn_params[0] to the forward pass\n",
    "    # of the first batch normalization layer, self.bn_params[1] to the forward\n",
    "    # pass of the second batch normalization layer, etc.\n",
    "    self.bn_params = []\n",
    "    if self.use_batchnorm:\n",
    "      self.bn_params = [{'mode': 'train'} for i in np.arange(self.num_layers - 1)]\n",
    "    \n",
    "    # Cast all parameters to the correct datatype\n",
    "    for k, v in self.params.items():\n",
    "      self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "  def loss(self, X, y=None):\n",
    "    \"\"\"\n",
    "    Compute loss and gradient for the fully-connected net.\n",
    "\n",
    "    Input / output: Same as TwoLayerNet above.\n",
    "    \"\"\"\n",
    "    X = X.astype(self.dtype)\n",
    "    mode = 'test' if y is None else 'train'\n",
    "\n",
    "    # Set train/test mode for batchnorm params and dropout param since they\n",
    "    # behave differently during training and testing.\n",
    "    if self.dropout_param is not None:\n",
    "      self.dropout_param['mode'] = mode   \n",
    "    if self.use_batchnorm:\n",
    "      for bn_param in self.bn_params:\n",
    "        bn_param[mode] = mode\n",
    "\n",
    "    scores = None\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Implement the forward pass of the FC net and store the output\n",
    "    #   scores as the variable \"scores\".\n",
    "    # ================================================================ #\n",
    "\n",
    "    H = []\n",
    "    cache_h = []\n",
    "    for i in np.arange(1,self.num_layers + 1):\n",
    "        name_W = 'W'+str(i)\n",
    "        name_b = 'b'+str(i)\n",
    "\n",
    "        if i == 1:                   \n",
    "            H.append(affine_relu_forward(X, self.params[name_W], self.params[name_b])[0])\n",
    "            cache_h.append(affine_relu_forward(X, self.params[name_W], self.params[name_b])[1])\n",
    "        elif i == self.num_layers:          \n",
    "            scores = affine_forward(H[i-2], self.params[name_W], self.params[name_b])[0]\n",
    "            cache_h.append(affine_forward(H[i-2], self.params[name_W], self.params[name_b])[1])\n",
    "        else:    \n",
    "            H.append(affine_relu_forward(H[i-2], self.params[name_W], self.params[name_b])[0])\n",
    "            cache_h.append(affine_relu_forward(H[i-2], self.params[name_W], self.params[name_b])[1])\n",
    "                                                                                        \n",
    "\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "    \n",
    "    # If test mode return early\n",
    "    if mode == 'test':\n",
    "      return scores\n",
    "\n",
    "    loss, grads = 0.0, {}\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Implement the backwards pass of the FC net and store the gradients\n",
    "    #   in the grads dict, so that grads[k] is the gradient of self.params[k]\n",
    "    #   Be sure your L2 regularization includes a 0.5 factor.\n",
    "    # ================================================================ #\n",
    "\n",
    "    loss, dz = softmax_loss(scores, y)\n",
    "    dh = []\n",
    "    for i in np.arange(self.num_layers,0,-1):\n",
    "        name_W = 'W'+str(i)\n",
    "        name_b = 'b'+str(i)\n",
    "        loss += (0.5 * self.reg * np.sum(self.params[name_W]*self.params[name_W]))\n",
    "        \n",
    "        if i == self.num_layers:\n",
    "            dh1, grads[name_W], grads[name_b] = affine_backward(dz, cache_h[self.num_layers-1])\n",
    "            dh.append(dh1)\n",
    "        else:\n",
    "            dh1, grads[name_W], grads[name_b] = affine_relu_backward(dh1, cache_h[i-1])\n",
    "            dh.append(dh1)\n",
    "    \n",
    "        grads[name_W] += self.reg * self.params[name_W]\n",
    "\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "    return loss, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
